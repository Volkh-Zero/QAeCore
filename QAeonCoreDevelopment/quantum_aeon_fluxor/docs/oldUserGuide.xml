<?xml version="1.0" encoding="UTF-8" ?>
<UserGuide project='Quantum Aeon Fluxor' version='0.1.0'>
    <Overview>
        <Purpose>
    The Quantum Aeon Fluxor (QAF) is an AI research framework focused on consciousness, meta-cognition,
    and emergent insight. It implements a conversational Archon agent (Gemini-based), a prompt framework
    (Syzygy), persistent memory (Hermetic Engine), and retrieval via Qdrant vector search.
    </Purpose>
        <DesignPrinciples>
            <Principle>Python-first, uv-managed</Principle>

            <Principle>Import-safe names with mythic display labels</Principle>

            <Principle>Transparent, persistent memory (episodic + long-term)</Principle>

            <Principle>Extensible agents and tools (MCP, retrievers, adversaries)</Principle>
        </DesignPrinciples>
    </Overview>

    <Naming>
        <Convention>
            <ImportSafe>Words in lowercase with underscores; parentheses replaced by double underscores.</ImportSafe>
            <Example original='Syzygy(ConversationalFramework)'>syzygy__conversational_framework</Example>
        </Convention>
        <DisplayNames>
    Mythic display names are preserved via __display_name__ and myth_map.json.
    Use display.py::display_name(import_safe) to render titles.
    </DisplayNames>
    </Naming>

    <RepositoryLayout root='QAeonCoreDevelopment/quantum_aeon_fluxor'>
        <Package name='archon__supervisor_agent' display='Archon (SuperVisor Agent)'>
            <File>archon.py</File>

            <File>state.py</File>

            <Note>Orchestrates conversation loop, persists state.</Note>
        </Package>

        <Package name='syzygy__conversational_framework' display='Syzygy (ConversationalFramework)'>
            <File>Integration_Prototyping/qacore_prompt_engine.py</File>

            <File>bridge.py</File>

            <Note>Prompt kernels and adapter used by Archon.</Note>
        </Package>

        <Package name='hermetic_engine__persistent_data' display='Hermetic Engine (Persistent Data)'>
            <File>emergent_chironomicon__coherent_vectors/memory_logger.py</File>

            <File>conduits__clients/Gemini/GeminiClient.py</File>

            <File>longterm.py</File>

            <File>embedding/gemini_embedder.py</File>

            <File>retrieval/qdrant_store.py</File>

            <File>indexing/index_folder.py</File>

            <Note>Memory, clients, embeddings, retrieval, and indexing tools.</Note>
        </Package>

        <Package name='pleroma__reasoning' display='Pleroma (Reasoning)'>
            <File>BaseStrategy.py</File>
            <File>LogicalFlowStrategy.py</File>
        </Package>

        <RootFiles>
            <File>main.py</File>

            <File>myth_map.json</File>

            <File>display.py</File>

            <File>qacore_gemini_integration.py</File>
        </RootFiles>
    </RepositoryLayout>

    <Dependencies manager='uv'>
        <Pyproject path='QAeonCoreDevelopment/pyproject.toml'>
            <Runtime>
                <Package>google-generativeai</Package>

                <Package>python-dotenv</Package>

                <Package>pydantic</Package>

                <Package>qdrant-client</Package>
            </Runtime>

            <Dev>
                <Package>pytest</Package>

                <Package>ruff</Package>

                <Package>black</Package>
            </Dev>

            <Scripts>
                <Script name='qaf-cli' entry='quantum_aeon_fluxor.main:main' />
                <Script name='qaf-index' entry='quantum_aeon_fluxor.hermetic_engine__persistent_data.indexing.index_folder:index_folder' />
            </Scripts>
        </Pyproject>
    </Dependencies>

    <EnvironmentVariables>
        <Var name='GOOGLE_API_KEY' required='true'>Google AI Studio key for Gemini (chat + embeddings).</Var>

        <Var name='GEMINI_EMBED_MODEL' required='false'>Defaults to gemini-embedding-001 (3072 dims).</Var>

        <Var name='QDRANT_URL' required='true'>Qdrant Cloud REST endpoint including :6333.</Var>

        <Var name='QDRANT_API_KEY' required='true'>Qdrant Cloud API key.</Var>
    </EnvironmentVariables>

    <CLI>
        <Command name='qaf-cli'>
            <Mode name='basic'>Single prompt demo.</Mode>

            <Mode name='demo'>QAeCore prompt showcase.</Mode>

            <Mode name='conversation'>Interactive Archon REPL. Type 'exit' or 'quit' to leave.</Mode>
        </Command>
        <Command name='qaf-index'>
            <Description>Index a folder into Qdrant using Gemini embeddings.</Description>

            <Args>
                <Arg name='folder' required='true'>Folder path to index.</Arg>

                <Arg name='--collection' default='qaecore_longterm_v1'>Qdrant collection name.</Arg>

                <Arg name='--max-chars' default='2000'>Chunk size in characters.</Arg>

                <Arg name='--overlap' default='200'>Chunk overlap.</Arg>
            </Args>

            <Example>qaf-index C:\\Users\\kayno\\QAeCore\\QAeonCoreDevelopment\\quantum_aeon_fluxor\\hermetic_engine__persistent_data --collection qaecore_longterm_v1</Example>
        </Command>
    </CLI>

    <Archon>
    <State file='hermetic_engine__persistent_data/Mnemosyne_Engine(State_Machine)/state/archon_state.json' legacy='hermetic_engine__persistent_data/state/archon_state.json'>
            <Field>phase</Field>

            <Field>focus_topic</Field>

            <Field>open_questions</Field>

            <Field>working_hypotheses</Field>

            <Field>contradictions</Field>

            <Field>bias_flags</Field>

            <Field>insight_candidates</Field>
        </State>

        <Transcript>
            <Mechanism>episodic_log_interaction(speaker, text) writes Markdown entries to a session log.</Mechanism>
            <Location>emergent_chironomicon__coherent_vectors/memory_logger.py</Location>
        </Transcript>

        <LongTermMemory>
            <Mechanism>write_blob(blob, tags, meta) stores opaque text with metadata sidecar.</Mechanism>
            <Location>hermetic_engine__persistent_data/longterm.py</Location>
        </LongTermMemory>

        <Prompting>
            <Adapter>syzygy__conversational_framework/bridge.py</Adapter>
            <Engine>Integration_Prototyping/qacore_prompt_engine.py</Engine>
        </Prompting>

        <Model>
            <Client>conduits__clients/Gemini/GeminiClient.py</Client>
            <ModelName>gemini-2.5-pro</ModelName>
        </Model>
    </Archon>

    <VectorRetrieval>
        <Qdrant>
            <Client>qdrant-client</Client>

            <Endpoint note='must include :6333' />

            <Auth>QDRANT_API_KEY</Auth>

            <Collection>
                <Default>qaecore_longterm_v1</Default>

                <VectorSize>3072</VectorSize>

                <Distance>COSINE</Distance>
            </Collection>
        </Qdrant>

        <Embeddings>
            <Provider>Gemini</Provider>

            <Model>gemini-embedding-001</Model>

            <Dimension>3072</Dimension>

            <Module>embedding/gemini_embedder.py</Module>
        </Embeddings>

        <Indexing>
            <Module>indexing/index_folder.py</Module>

            <Chunking max_chars='2000' overlap='200' />

            <Payload>path, rel_path, chunk_index</Payload>
        </Indexing>
    </VectorRetrieval>

    <Security>
        <SecretHandling>
    Do not commit API keys. Use environment variables or .env (for local dev).
    The repository contains redaction markers where applicable; never copy them into code.
    </SecretHandling>
    </Security>

    <Roadmap>
        <Planned>
            <Item>Conversation commands (:retain, :state, :search)</Item>

            <Item>Qdrant search CLI and Archon retrieval hook</Item>

            <Item>Adversarial agent harness</Item>

            <Item>MCP integration for persistent docs server</Item>
        </Planned>
    </Roadmap>

    <ChangeLog>
        <Entry date='2025-08-13'>
            <Summary>Initial scaffold: Archon, state, memory, Syzygy bridge, Qdrant indexing, CLI.</Summary>
        </Entry>
    </ChangeLog>
</UserGuide>